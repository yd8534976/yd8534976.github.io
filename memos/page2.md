**常用激活函数(Activation function)**

-ReLU(Rectified linear unit)
 f(x)=max(0,x)

-Softmax(multiclass classification)
 \sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}    for j = 1, …, K.
 loss function: cross-entropy