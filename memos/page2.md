## **常用激活函数(Activation function)**

### 1. ReLU(Rectified linear unit)

- f(x)=max(0,x)

### 2. Softmax(multiclass classification)

- ![Image of Softmax](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)  for j = 1, …, K.

- loss function: cross-entropy